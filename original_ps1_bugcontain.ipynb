{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWjR-pgsd-Si"
   },
   "source": [
    "# CS640 Programming Assignment 1: Neural Network\n",
    "\n",
    "In this programming assignment, you are asked to construct a neural network model (almost) from scratch, run experiments, and write reports. We provide a script of skeleton code as well as three datasets.\n",
    "\n",
    "Your tasks are the following.\n",
    "1. Build your network model following the instruction.\n",
    "2. Run experiments and produce results.\n",
    "3. Interpret and discuss your results.\n",
    "\n",
    "## Submission\n",
    "Everything you need to complete for this assignment is in this notebook, so this file is the only thing to submit via Gradescope before the deadline.\n",
    "\n",
    "## Collaboration\n",
    "You are allowed to work in a team of at most **three** on the **Q1: Coding** part, but you must run the experiments and write the reports independently.\n",
    "\n",
    "## More instructions\n",
    "\n",
    "If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n",
    "\n",
    "In an ipython notebook, to run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctrl+Enter` or `[>|]`(like \"play\") button above. To edit any code or text cell (double) click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above.\n",
    "\n",
    "To enter your solutions for the written questions, put down your derivations into the corresponding cells below using LaTeX. Show all steps when proving statements. If you are not familiar with LaTeX, you should look at some tutorials and at the examples listed below between \\$..\\$. We will not accept handwritten solutions.\n",
    "\n",
    "Put your solutions into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. (Double) click on a cell to edit or to see its source code. You can add cells via **`+`** sign at the top left corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9ImiOu7jzNu"
   },
   "source": [
    "**Q0: What's your name? If you are collaborating with someone, please list their names here as well.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4ldlvqYkyKw"
   },
   "source": [
    "**Lingyan Jiang**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6AxRuFOlU_B"
   },
   "source": [
    "**Q1: Coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce5gxnBCm7HW"
   },
   "source": [
    "**Q1.1: Import Packages**\n",
    "\n",
    "The packages that have been imported in the following block should be sufficient for this assignment, but you are free to add more if necessary. However, keep in mind that you **should not** import and use any neural network package. If you have concern about an addition package, please contact us via Piazza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rLhHI0rOdQJ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os, sys\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vI0x356RyO0h"
   },
   "source": [
    "**Q1.2: Load Data**\n",
    "\n",
    "We provide two sets of data, namely dataset1 and dataset2. Dataset1 contains two subsets of data, namely linearly and nonlinearly, neither of which is splitted into training and testing; while dataset2 has been splitted into training and testing. Write functions here to load them in your favorable way. You may want to skip this part first and come back after reading the experimenting part below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a9SifEwQzEo7"
   },
   "outputs": [],
   "source": [
    "def getDataset1(dataDir, is_linear = True):\n",
    "    if is_linear:\n",
    "        samples = pd.read_csv(dataDir + 'LinearX.csv', index_col=False, header=None)\n",
    "        X = samples.values\n",
    "        label = pd.read_csv(dataDir + 'LinearY.csv', index_col=False, header=None)\n",
    "        Y = label.values\n",
    "    else:\n",
    "        samples = pd.read_csv(dataDir + 'NonlinearX.csv', index_col=False, header=None)\n",
    "        X = samples.values\n",
    "        label = pd.read_csv(dataDir + 'NonlinearY.csv', index_col=False, header=None)\n",
    "        Y = label.values\n",
    "    return X, Y\n",
    "\n",
    "def getDataset2(dataDir):\n",
    "    samples = pd.read_csv(dataDir + 'Digit_X_train.csv', index_col=False, header=None)\n",
    "    X = samples.values\n",
    "    label = pd.read_csv(dataDir + 'Digit_y_train.csv', index_col=False, header=None)\n",
    "    Y = label.values\n",
    "    return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9TiTFf9psFl"
   },
   "source": [
    "**Q1.3: Define Activation and Loss Functions**\n",
    "\n",
    "Complete the following functions. The ones starting with a \"d\" is the derivative of the corresponding function. You are free to implement additional ones (for example, softmax and cross-entory loss) if interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yuxWGvbhp5jD"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    sigmoidx = 1/(1 + np.exp(-x))\n",
    "    return sigmoidx\n",
    "    \n",
    "def dSigmoid(x):\n",
    "    dsigmoidx = (np.exp(-x))/((1 + np.exp(-x)) ** 2)\n",
    "    return dsigmoidx\n",
    "    \n",
    "def euclideanLoss(YTrue, YPredict):\n",
    "    YTrue = np.array(YTrue)\n",
    "    YPredict = np.array(YPredict)\n",
    "    loss = 0\n",
    "    for i in range(len(YTrue)):\n",
    "        sum = 0.5 * np.square(YPredict[i] - YTrue[i])\n",
    "        loss = loss + sum \n",
    "    return loss\n",
    "    \n",
    "def dEuclideanLoss(YTrue, YPredict):\n",
    "    YTrue = np.array(YTrue)\n",
    "    YPredict = np.array(YPredict)\n",
    "    loss = 2 * (YPredict - YTrue)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "[ 2  2  0 -2]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "a = [3,4,6,5]\n",
    "b = [4,5,6,4]\n",
    "print(euclideanLoss(a,b))\n",
    "print(dEuclideanLoss(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvhPU4YqoVZF"
   },
   "source": [
    "**Q1.4: Define the Layer Class**\n",
    "\n",
    "Complete the ***initializeWeights*** function, which initializes the weights and biases with small random values. The ***\\_\\_init\\_\\_*** function should be left as it is, though you are free to modify if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6r5h0hT8ofqQ"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, NInput, NOutput, bias = True):\n",
    "        self.NInput = NInput\n",
    "        self.NOutput = NOutput\n",
    "        self.useBias = bias\n",
    "        self.initializeWeights()\n",
    "    \n",
    "    def initializeWeights(self):\n",
    "        #Initializes the weights and biases with small random values.\n",
    "        self.weights = np.random.randn(self.NInput, self.NOutput) / np.sqrt(self.NInput)  \n",
    "        self.bias = np.zeros((1, self.NOutput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXcQsl_XpAOW"
   },
   "source": [
    "**Q1.5: Define the Network Class**\n",
    "\n",
    "This is the heaviest part of this assignment. We recommend you to first go over the math carefully before starting this part.\n",
    "\n",
    "In addition, you are strongly encouraged to use numpy for matrix operations. When doing multiplication, please be careful about the dimensions, as well as the difference between the \"*\" operator, numpy's ***multiply*** function, and numpy's ***dot*** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-p1xdHw0pKr8"
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers, activationList, dActivationList, loss, dLoss):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        layers : List[Layers]\n",
    "            This should be a list of Layers objects.\n",
    "        activationList/dActivationList: List[functions]\n",
    "            This should be a list of activation/derivative functions (e.g. sigmoid/dSigmoid).\n",
    "        loss/dLoss: function\n",
    "            This should be a loss/derivative function (e.g. enclideanLoss/dEuclideanLoss).\n",
    "        \"\"\"\n",
    "        assert len(layers) == len(activationList) == len(dActivationList)\n",
    "        self.layers = layers\n",
    "        self.activationList = activationList\n",
    "        self.dActivationList = dActivationList\n",
    "        self.loss = loss\n",
    "        self.dLoss = dLoss\n",
    "    \n",
    "                \n",
    "    def fit(self, X, Y, learningRate, regLambda):\n",
    "        \"\"\"\n",
    "        Fit the model with input features and targets.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X, Y : array-like\n",
    "            X contains the input features while Y contains the target values.\n",
    "        learningRate, regLambda : float\n",
    "            Basic hyperparameters for the model.\n",
    "        \"\"\"\n",
    "        # First, initialize zero gradients.\n",
    "        \n",
    "        # Next, use the forward and backprog functions to accumulate gradients\n",
    "        # sample by sample.\n",
    "        # Hint:\n",
    "        # For each sample, do:\n",
    "        #     Forward pass once, using the forward function defined below.\n",
    "        #     Backward propagation, using the backprog function defined below.\n",
    "        \n",
    "        # Lastly, update weights and biases using the gradients; don't forget to\n",
    "        # take the mean before updating the weights. Note that both the learning\n",
    "        # rate and the regularization should appear here.\n",
    "            \n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the outcome of given input features. This should be as easy as\n",
    "        doing foward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "        \"\"\"\n",
    "        for layer, activation in zip(self.layers, self.activationList):\n",
    "            Z=np.dot(X,layer.weights)+layer.bias;\n",
    "            X=activation(Z)\n",
    "        predictions=X    \n",
    "        return predictions\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward propagation for ONE sample.\n",
    "        x  : array-like\n",
    "            Note that this is just ONE single sample.\n",
    "        \"\"\"\n",
    "        i=1\n",
    "        value={}\n",
    "        value[0]=x\n",
    "        for layer, activation in zip(self.layers, self,activationList):\n",
    "            z=np.dot(x,layer.weights)+layer.bias\n",
    "            x=activation(z)\n",
    "            value[i]=x\n",
    "            i=i+1\n",
    "        return x, value\n",
    "        \n",
    "    def backprog(self, all_layers, y):\n",
    "        \"\"\"\n",
    "        Performs the backward propagation. This is the core part of the model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        None, but feel free to add anything you like.\n",
    "        \"\"\"\n",
    "        # First, compute the derivative of the loss and the output activation.\n",
    "        n=len(self.layers)\n",
    "        res={}\n",
    "        ypredict,value=self.forward(x)\n",
    "        dloss=dEuclideanLoss(y,ypredict)\n",
    "        dactivation=dSigmoid(ypredict)\n",
    "        layerList=self.layers\n",
    "        res[2*n]=np.multiply(dloss,dactivation)\n",
    "        res[2*n - 1]=np.dot(res[2*n],value[n-1])\n",
    "        # Then, compute the gradient layer by layer in the reverse order.\n",
    "        for index in range(1,n):\n",
    "            res[2*index]=np.multiply(np.dot(layerList[index].weights.T,res[2*(index+1)]),dSigmoid(value[index]))\n",
    "            res[2*index - 1]=np.dot(res[2*index],value[index-1])\n",
    "        return res\n",
    "    \n",
    "    def getLoss(self, X, Y, regLambda = 0):\n",
    "        \"\"\"\n",
    "        Compute and return the loss given X and Y. This function is helpful for\n",
    "        you to visualize the training process.\n",
    "        \"\"\"\n",
    "        YTrue =Y\n",
    "        YPredict = self.predict(X)\n",
    "        J0 = 0.5 * (YTrue - YPredict) * (YTrue - YPredict)\n",
    "        for layer in self.layers[::-1]:\n",
    "            J1 = regLambda * np.linalg.norm(layer.weights, ord=2)\n",
    "        J = J0 + J1\n",
    "        return J\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2oGlVbmwJw7"
   },
   "source": [
    "**Q1.6: Define Evaluation Functions**\n",
    "\n",
    "Complete the functions below following the instructions in the comments. **Do not** use an existing library. If you would like to include a ROC analysis in your report, please add the corresponding function in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3jW5iEBzwOgK"
   },
   "outputs": [],
   "source": [
    "def getConfusionMatrix(YTrue, YPredict):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    YTrue : numpy array\n",
    "        This array contains the ground truth.\n",
    "    YPredict : numpy array\n",
    "        This array contains the predictions.\n",
    "    Returns\n",
    "    CM : numpy matrix\n",
    "        The confusion matrix.\n",
    "    \"\"\"\n",
    "    YTrue = YTrue.T.flatten()\n",
    "    YPredict = YPredict.T.flatten()\n",
    "    true = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for i in range(len(YTrue)):\n",
    "        if YTrue[i] == YPredict[i] and YTrue[i] == 1:\n",
    "            tp = tp + 1\n",
    "        elif YTrue[i] == YPredict[i] and YTrue[i] == 0:\n",
    "            tn = tn + 1\n",
    "        elif YTrue[i] != YPredict[i] and YTrue[i] == 0:\n",
    "            fp = fp + 1\n",
    "        elif YTrue[i] != YPredict[i] and YTrue[i] == 1:\n",
    "            fn = fn + 1\n",
    "    CM = np.array([[tp, fn], [fp, tn]])\n",
    "    return CM\n",
    "\n",
    "    \n",
    "def getPerformanceScores(YTrue, YPredict):\n",
    "    \"\"\"\n",
    "    Computes the accuracy, precision, recall, f1 score.\n",
    "    Parameters\n",
    "    ----------\n",
    "    YTrue : numpy array\n",
    "        This array contains the ground truth.\n",
    "    YPredict : numpy array\n",
    "        This array contains the predictions.\n",
    "    Returns\n",
    "    {\"CM\" : numpy matrix,\n",
    "    \"accuracy\" : float,\n",
    "    \"precision\" : float,\n",
    "    \"recall\" : float,\n",
    "    \"f1\" : float}\n",
    "        This should be a dictionary.\n",
    "    \"\"\"\n",
    "    CM = getConfusionMatrix(YTrue, YPredict)\n",
    "    accuracy = (CM[0][0] + CM[1][1]) / (CM[0][0] + CM[0][1] + CM[1][0] + CM[1][1])\n",
    "    precision = (CM[0][0]) / (CM[1][0] + CM[0][0])\n",
    "    recall = CM[0][0] / (CM[0][0] + CM[0][1])\n",
    "    f1 = 2 / (1 / precision + 1 / recall)\n",
    "    return CM, accuracy, precision, recall, f1\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NS8f9qz0fSK"
   },
   "source": [
    "**Q1.7: Test Model**\n",
    "\n",
    "Use the following example code to test your model with some simple data. You may want to go back to **Q1.2** if you skipped it earlier.\n",
    "\n",
    "Hint:\n",
    "1. Changing your model structure can help you discover bugs in implementation.\n",
    "2. The loss plot should help you make sure that your model is indeed learning.\n",
    "\n",
    "**You need to produce decreasing loss curves here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gTyLlzEa0a0U"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2-D, but have shapes (20,) and (20, 400, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7cc276feb0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# plot the losses, both curves should be decreasing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2761\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m   2763\u001b[0m         is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             raise ValueError(f\"x and y can be no greater than 2-D, but have \"\n\u001b[0m\u001b[1;32m    346\u001b[0m                              f\"shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y can be no greater than 2-D, but have shapes (20,) and (20, 400, 1)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = getDataset1('/Users/lingyanjiang/study/cs640/P1/Data/dataset1/',is_linear=True)# change this line accordingly\n",
    "XTrain, XTest, YTrain, YTest = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# assemble your model\n",
    "layers = [Layer(2, 4), Layer(4, 1)]\n",
    "model = Network(layers, [sigmoid, sigmoid], [dSigmoid, dSigmoid], euclideanLoss, dEuclideanLoss)\n",
    "\n",
    "# specify training parameters\n",
    "epochs = 20\n",
    "learningRate = 1e-2\n",
    "regLambda = 0\n",
    "\n",
    "# capture the loss values during training\n",
    "loss = {\"train\" : [0.0] * epochs, \"test\" : [0.0] * epochs}\n",
    "\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    model.fit(XTrain, YTrain, learningRate, regLambda)\n",
    "    loss[\"train\"][epoch] = model.getLoss(XTrain, YTrain, regLambda)\n",
    "    loss[\"test\"][epoch] = model.getLoss(XTest, YTest, regLambda)\n",
    "\n",
    "# plot the losses, both curves should be decreasing\n",
    "plt.plot([i for i in range(epochs)], loss[\"train\"], label = \"train\")\n",
    "plt.plot([i for i in range(epochs)], loss[\"test\"], label = \"test\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Plot\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs8PtEHi2jXs"
   },
   "source": [
    "**Q2: Experimenting with Dataset 1**\n",
    "\n",
    "In this section, the **performance results** you need to report include the following:\n",
    "1. confusion matrix;\n",
    "2. accuracy;\n",
    "3. precision;\n",
    "4. recall;\n",
    "5. F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfMeuvrO3RaU"
   },
   "source": [
    "**Q2.1: LinearXY**\n",
    "\n",
    "Split the data using [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Train your model, evaluate the prediction on the testing subset, and report the **performance results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gDONUUH37ooO"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d67357514589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# write your code in this block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# write your code in this block\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5y4l2tA7rGv"
   },
   "source": [
    "In addition, use the provided ***plotDecisionBoundary*** function to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "YJVa8Po973V7"
   },
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(model, X, Y):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary given by model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : model, whose parameters are used to plot the decision boundary.\n",
    "    X : input data\n",
    "    Y : input labels\n",
    "    \"\"\"\n",
    "    x1_array, x2_array = np.meshgrid(np.arange(-4, 4, 0.01), np.arange(-4, 4, 0.01))\n",
    "    grid_coordinates = np.c_[x1_array.ravel(), x2_array.ravel()]\n",
    "    Z = model.predict(grid_coordinates)\n",
    "    Z = Z.reshape(x1_array.shape)\n",
    "    plt.contourf(x1_array, x2_array, Z, cmap=plt.cm.bwr)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr)\n",
    "    plt.show()\n",
    "#continue writing your code in this block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxFyc5kI8D-I"
   },
   "source": [
    "**Q2.2: NonLinearXY**\n",
    "\n",
    "For the experiments done in this subset of the data, apply stratified 5-fold cross-validation (use the [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) module from scikit-learn), and report the mean **performance results** across the folds.\n",
    "\n",
    "You should produce and describe in words\n",
    "1. **performance results** of at least **5** different choices of total epochs;\n",
    "2. **performance results** of at least **5** different choices of learning rate;\n",
    "3. **performance results** of at least **3** different choices of regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGasEudB__Dh"
   },
   "source": [
    "**Q3: Experimenting with Dataset 2**\n",
    "\n",
    "Train your model, evaluate the prediction on the testing subset, report and describe in words\n",
    "1. **performance results** of at least **5** different choices of total epochs;\n",
    "2. **performance results** of at least **5** different choices of learning rate;\n",
    "3. **performance results** of at least **3** different choices of regularization parameter.\n",
    "4. **performance results** of at least **5** different choices of number of nodes in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54PIDOy8Aqbx"
   },
   "source": [
    "**Q4: Other Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPFklnbuAwg_"
   },
   "source": [
    "**Q4.1** Briefly describe the workflow of how your model classify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8RYC8yiBEvU"
   },
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6k2vbJBFcU"
   },
   "source": [
    "**Q4.2** In your own words, explain how the forward propagation in your model works. You can cite your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZSqrQX-BofT"
   },
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDkm7LCOBpOZ"
   },
   "source": [
    "**Q4.3** In your own words, explain how the backward propagation in your model works. You can cite your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCf69EeTB37F"
   },
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4v_t-JdB726"
   },
   "source": [
    "**Q4.4** In theory, how do the total number of epochs, the learning rate, and the regularization parameter impact the performance of model? Does any of the theoretical impact actually happen in your result? If so, point them out."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS640P1NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
